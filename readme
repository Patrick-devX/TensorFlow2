############################
# Book hands On Machine Learning with Scikit Leaarn, Keras and TensorFlow2

# Chapter 10. Introduction to Artificial Neural Networks with Keras
McCulloch and Pitts proposed a very simple model of the biological neuron, which later become known as an artificial neuron:
it has one or more binary (on/off) inputs and one binary output. The artificial neuron activates its output when more than a certain number of its inputs are active.

The Perceptron is one of the simplest ANN architectures. It is based on a slightly different artificial neuron called a THRESHOLD LOGIC UNIT (TLU), or sometimes
a LINEAR THRESHOLD UNIT(LTU). A Perceptron is simply composed of a single layer of TLU, which each TLU connected to all the inputs. When all the neurons in a layer are
connected to every neuron in the previous layer, the layer is called fully connected layer , or a Dense layer.
The Perceptrons are trained using a variant of this rule that takes into account the error made by the network when makes a prediction; the perceptron learning rule
reinforces connections that help reduce the error, More specially, the Perceptron is fed one training instance time, and for each instance it makes its predictions

Scikit Learn prvides a Perceptron class that implements a single TLU (Threshold Logic Unit) network. In fact Scikit Learn's Perceptron class is equivalent to using
an SGDClassifier with the following hyperparameters: loss="perceptron", learning_rate="constant" with no regularizations.

Contrary to Logistic regression Classifiers, Perceptrons do not output a class probability; rather, they make predictions based on a hard threshold. This is the one reason
to prefer Logistic Regression over Perceptrons

In their 1969 monograph Perceptrons, Marvin Minsky and Seymour Papert highlighted a number of serous weaknesses of Perceptrons---in particular, the fact that
they are incapable of solving some trivial problems (e. g. the Exclusive Or(XOR)) classification problem. It turns that some of the limitations of the Perceptrons
 can be eliminated by stacking multiple Perceptrons. The resulting ANN is called a Multilayer Perceptron (MLP). An MLP can solve the XOR Problem.

 ## The Multilayer Perceptron and backpropagation
 An MLP is composed of one (passthrough) input Layer, one or more Layers of TLU (Theshold Logic Unit), called hidden Layers and one final layer of TLUs called the output layer.
 The signal flows only in one direction (from the inputs to the outputs), so this architechture is an example of feedfirward neural network (FNN)
 When an ANN contains a deep stack of hidden layers, it is called a deep neural network (DNN).

 MLPs are trained using the backpropagation training algorithm. In short ist the Gradient Descent.


 Implementing MLPs with keras
 Since 2016, other implementations have been released. You can now run Keras on Apache MXNET, Appele Core ML, JavaScript or TypeScript(to run Keras in a Web browser)

 ########################################################################

 Building an Image Classifier Using the Sequential API  imageClassifier1.py

 * Flatten Layer: Convert each imput Image into 1D array
 * High number of weights gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfiting,
  especially when you do not have a lot of training data
 * sparse_categorical_crossentropy: loss because we have sparse labels (i.e., for each instance, there is just a target class index from 0 to 9 in this case)
    If instead we had one target probability per class for each instance ( such as one-hot vectors, e.g, [0.,0.,1.,0.,0.,0.,0.,0.,0.,0.] to represent 3),
    then we would need to use the categorical_crossentropy loss instead.
 * If we are doing binary classification ( with one or more binary labels) , then we would use the sigmoid ( i.e. logistic) activation function in the output layer
 instead of the softmax activation function, and we would use the binary_crossentropy
